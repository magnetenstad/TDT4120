
# Pensumhefte, 2021, realisert.

## Læringsmål

### Overordnede læringsmål
De overordnede læringsmålene for emnet er som følger.

**Dere skal ha kunnskap om:**
- [ ] [X1] Et bredt spekter av etablerte algoritmer og datastrukturer
- [ ] [X2] Klassiske algoritmiske problemer med kjente effektive løsninger
- [ ] [X3] Komplekse problemer uten kjente effektive løsninger

**Dere skal kunne:**
- [ ] [X4] Analysere algoritmers korrekthet og effektivitet
- [ ] [X5] Formulere problemer så de kan løses av algoritmer
- [ ] ! [X6] Konstruere nye effektive algoritmer

**Dere skal være i stand til:**
- [ ] [X7] Å bruke eksisterende algoritmer og programvare på nye problemer
- [ ] [X8] Å utvikle nye løsninger på praktiske algoritmiske problemstillinger

### Gjennom semesteret

**Læringsmål for hver algoritme:**
- [ ] [Z1] Kjenne den formelle definisjonen av det generelle problemet den løser
- [ ] [Z2] Kjenne til eventuelle tilleggskrav den stiller for å være korrekt
- [ ] [Z3] Vite hvordan den oppfører seg; kunne utføre algoritmen, trinn for trinn
- [ ] ! [Z4] Forstå korrekthetsbeviset; hvordan og hvorfor virker algoritmen egentlig?
- [ ] [Z5] Kjenne til eventuelle styrker eller svakheter, sammenlignet med andre
- [ ] [Z6] Kjenne kjøretidene under ulike omstendigheter, og forstå utregningen

**Læringsmål for hver datastruktur:**
- [ ] [Z7] Forstå algoritmene (jf. mål Z01–Z06) for de ulike operasjonene på strukturen
- [ ] [Z8] Forstå hvordan strukturen representeres i minnet

**Læringsmål for hvert problem:**
- [ ] [Z9] Kunne angi presist hva input er
- [ ] [Z10] Kunne angi presist hva output er og hvilke egenskaper det må ha

### Forelesning 1: Problemer og algoritmer
- [ ] [A1] [Forstå bokas pseudokode-konvensjoner](#a1-forstå-bokas-pseudokode-konvensjoner)
- [ ] [A2] [Kjenne egenskapene til random-access machine-modellen (RAM)](#a2-kjenne-egenskapene-til-random-access-machine-modellen-ram)
- [ ] [A3] [Kunne definere problem, instans og problemstørrelse](#a3-kunne-definere-problem-instans-og-problemstørrelse)
- [ ] ! [A4] [Kunne definere asymptotisk notasjon, O, Ω, Θ, o og ω](#-a4-kunne-definere-asymptotisk-notasjon-o-ω-θ-o-og-ω)
- [ ] ! [A5] [Kunne definere best-case, average-case og worst-case](#-a5-kunne-definere-best-case-average-case-og-worst-case)
- [ ] ! [A6] [Forstå løkkeinvarianter og induksjon](#-a6-forstå-løkkeinvarianter-og-induksjon)
- [ ] ! [A7] [Forstå rekursiv dekomponering og induksjon over delinstanser](#-a7-forstå-rekursiv-dekomponering-og-induksjon-over-delinstanser)
- [x] [A8] [Forstå Insertion-Sort](#a8-forstå-insertion-sort)

### Forelesning 2: Datastrukturer
- [x] [B1] [Forstå hvordan stakker og køer fungerer](#b1-forstå-hvordan-stakker-og-køer-fungerer)
- [x] [B2] [Forstå hvordan lenkede lister fungerer](#b2-forstå-hvordan-lenkede-lister-fungerer)
- [ ] [B3] [Forstå hvordan pekere og objekter kan implementeres](#b3-forstå-hvordan-pekere-og-objekter-kan-implementeres)
- [ ] ! [B4] [Forstå hvordan direkte adressering og hashtabeller fungerer](#-b4-forstå-hvordan-direkte-adressering-og-hashtabeller-fungererx)
- [x] [B5] [Forstå konfliktløsing ved kjeding (chaining)](#b5-forstå-konfliktløsing-ved-kjeding-chaining)
- [ ] [B6] [Kjenne til grunnleggende hashfunksjoner](#b6-kjenne-til-grunnleggende-hashfunksjoner)
- [ ] [B7] [Vite at man for statiske datasett kan ha worst-case O(1) for søk](#b7-vite-at-man-for-statiske-datasett-kan-ha-worst-case-o1-for-søk)
- [ ] [B8] [Kunne definere amortisert analyse](#b7-vite-at-man-for-statiske-datasett-kan-ha-worst-case-o1-for-søk)
- [x] [B9] [Forstå hvordan dynamiske tabeller fungerer](#b8-kunne-definere-amortisert-analyse)

### Forelesning 3: Splitt og hersk
- [ ] ! [C1] [Forstå designmetoden divide-and-conquer (splitt og hersk)](#-c1-forstå-designmetoden-divide-and-conquer-splitt-og-hersk)
- [x] [C2] [Forstå maximum-subarray-problemet med løsninger](#c2-forstå-maximum-subarray-problemet-med-løsninger)
- [x] [C3] [Forstå Bisect og Bisect'](#c3-forstå-bisect-og-bisect)
- [x] [C4] [Forstå Merge-Sort](#c4-forstå-merge-sort)
- [x] [C5] [Forstå Quicksortog Randomized-Quicksort](#c5-forstå-quicksort-og-randomized-quicksort)
- [ ] ! [C6] [Kunne løse rekurrenser med substitusjon, rekursjonstrær og masterteoremet](#-c6-kunne-løse-rekurrenser-med-substitusjon-rekursjonstrær-og-masterteoremet)
- [ ] ! [C7] [Kunne løse rekurrenser med iterasjonsmetoden](#-c7-kunne-løse-rekurrenser-med-iterasjonsmetoden)
- [ ] [C8] [Forstå hvordan variabelskifte fungerer](#c8-forstå-hvordan-variabelskifte-fungerer)

### Forelesning 4: Rangering i lineær tid
- [ ] ! [D1] [Forstå hvorfor sammenligningsbasert sortering har en worst-case på Ω(n lg n)](#-d1-forstå-hvorfor-sammenligningsbasert-sortering-har-en-worst-case-på-ωn-lg-n)
- [ ] [D2] [Vite hva en stabil sorteringsalgoritme er](#d2-vite-hva-en-stabil-sorteringsalgoritme-er)
- [x] [D3] [Forstå Counting-Sort, og hvorfor den er stabil](#d3-forstå-counting-sort-og-hvorfor-den-er-stabil)
- [x] ! [D4] [Forstå Radix-Sort, og hvorfor den trenger en stabil subrutine](#-d4-forstå-radix-sort-og-hvorfor-den-trenger-en-stabil-subrutine)
- [x] [D5] [Forstå Bucket-Sort](#d5-forstå-bucket-sort)
- [x] [D6] [Forstå Randomized-Select](#d6-forstå-randomized-select)
- [ ] [D7] [Kjenne til Select](#d7-kjenne-til-select)

### Forelesning 5: Rotfaste trestrukturer
- [x] ! [E1] [Forstå hvordan heaps fungerer, og hvordan de kan brukes som prioritetskøer](#-e1-forstå-hvordan-heaps-fungerer-og-hvordan-de-kan-brukes-som-prioritetskøer)
- [x] [E2] [Forstå Heapsort](#e2-forstå-heapsort)
- [ ] [E3] [Forstå hvordan rotfaste trær kan implementeres](#e3-forstå-hvordan-rotfaste-trær-kan-implementeres)
- [x] ! [E4] [Forstå hvordan binære søketrær fungerer](#-e4-forstå-hvordan-binære-søketrær-fungerer)
- [ ] [E5] [Vite at forventet høyde for et tilfeldig binært søketre er Θ(lg n)](#e5-vite-at-forventet-høyde-for-et-tilfeldig-binært-søketre-er-θlg-n)
- [ ] [E6] [Vite at det finnes søketrær med garantert høyde på Θ(lg n)](#e6-vite-at-det-finnes-søketrær-med-garantert-høyde-på-θlg-n)

### Forelesning 6: Dynamisk programmering
- [ ] ! [F1] [Forstå ideen om en delinstansgraf](#-f1-forstå-ideen-om-en-delinstansgraf)
- [ ] ! [F2] [Forstå designmetoden dynamisk programmering](#-f2-forstå-designmetoden-dynamisk-programmering)
- [ ] ! [F3] [Forstå løsning ved memoisering (top-down)](#-f3-forstå-løsning-ved-memoisering-top-down)
- [ ] [F4] [Forstå løsning ved iterasjon (bottom-up)](#f4-forstå-løsning-ved-iterasjon-bottom-up)
- [ ] [F5] [Forstå hvordan man rekonstruerer en løsning fra lagrede beslutninger](#f5-forstå-hvordan-man-rekonstruerer-en-løsning-fra-lagrede-beslutninger)
- [ ] [F6] [Forstå hva optimal delstruktur er](#f6-forstå-hva-optimal-delstruktur-er)
- [ ] [F7] [Forstå hva overlappende delinstanser er](#f7-forstå-hva-overlappende-delinstanser-er)
- [x] [F8] [Forstå eksemplene stavkutting og LCS](#f8-forstå-eksemplene-stavkutting-og-lcs)
- [x] [F9] [Forstå løsningen på det binære ryggsekkproblemet](#f9-forstå-løsningen-på-det-binære-ryggsekkproblemet)

### Forelesning 7: Grådige algoritmer
- [ ] ! [G1] [Forstå designmetoden grådighet](#-g1-forstå-designmetoden-grådighet)
- [ ] ! [G2] [Forstå grådighetsegenskapen (the greedy-choice property)](#-g2-forstå-grådighetsegenskapen-the-greedy-choice-property)
- [ ] [G3] [Forstå eksemplene aktivitet-utvelgelse og det kontinuerlige ryggsekkproblemet](#g3-forstå-eksemplene-aktivitet-utvelgelse-og-det-kontinuerlige-ryggsekkproblemet)
- [x] [G4] [Forstå Huffman og Huffman-koder](#g4-forstå-huffman-og-huffman-koder)

### Forelesning 8: Traversering av grafer
- [ ] [H1] [Forstå hvordan grafer kan implementeres](#h1-forstå-hvordan-grafer-kan-implementeres)
- [x] [H2] [Forstå BFS, også for å finne korteste vei uten vekter](#h2-forstå-bfs-også-for-å-finne-korteste-vei-uten-vekter)
- [x] [H3] [Forstå DFS, parentesteoremet og hvit-sti-teoremet](#h3-forstå-dfs-parentesteoremet-og-hvit-sti-teoremet)
- [ ] [H4] [Forstå hvordan DFS klassifiserer kanter](#h4-forstå-hvordan-dfs-klassifiserer-kanter)
- [x] [H5] [Forstå Topological-Sort](#h5-forstå-topological-sort)
- [ ] [H6] [Forstå hvordan DFS kan implementeres med en stakk](#h6-forstå-hvordan-dfs-kan-implementeres-med-en-stakk)
- [ ] [H7] [Forstå hva traverseringstrær (som bredde-først- og dybde-først-trær) er](#h7-forstå-hva-traverseringstrær-som-bredde-først--og-dybde-først-trær-er)
- [ ] ! [H8] [Forstå traversering med vilkårlig prioritetskø](#-h8-forstå-traversering-med-vilkårlig-prioritetskø)

### Forelesning 9: Minimale spenntrær
- [x] [I1] [Forstå skog-implementasjonen av disjunkte mengder](#i1-forstå-skog-implementasjonen-av-disjunkte-mengder)
- [ ] [I2] [Vite hva spenntrær og minimale spenntrær er](#i2-vite-hva-spenntrær-og-minimale-spenntrær-er)
- [x] ! [I3] [Forstå Generic-MST](#-i3-forstå-generic-mst)
- [ ] [I4] [Forstå hvorfor lette kanter er trygge kanter](#i4-forstå-hvorfor-lette-kanter-er-trygge-kanter)
- [x] [I5] [Forstå MST-Kruskal](#i5-forstå-mst-kruskal)
- [x] [I6] [Forstå MST-Prim](#i6-forstå-mst-prim)

### Forelesning 10: Korteste vei fra én til alle
- [ ] [J1] [Forstå ulike varianter av korteste-vei- eller korteste-sti-problemet](#j1-forstå-ulike-varianter-av-korteste-vei--eller-korteste-sti-problemet-single-source-single-destination-single-pair-all-pairs)
- [ ] [J2] [Forstå strukturen til korteste-vei-problemet](#j2-forstå-strukturen-til-korteste-vei-problemet)
- [ ] [J3] [Forstå at negative sykler gir mening for korteste enkle vei (simple path)](#j3-forstå-at-negative-sykler-gir-mening-for-korteste-enkle-vei-simple-path)
- [ ] [J4] [Forstå at korteste enkle vei kan løses vha. lengste enkle vei og omvendt](#j4-forstå-at-korteste-enkle-vei-kan-løses-vha-lengste-enkle-vei-og-omvendt)
- [ ] [J5] [Forstå hvordan man kan representere et korteste-vei-tre](#j5-forstå-hvordan-man-kan-representere-et-korteste-vei-tre)
- [x] ! [J6] [Forstå kant-slakking (edge relaxation) og Relax](#-j6-forstå-kant-slakking-edge-relaxation-og-relax)
- [ ] [J7] [Forstå ulike egenskaper ved korteste veier og slakking](#j7-forstå-ulike-egenskaper-ved-korteste-veier-og-slakking)
- [x] [J8] [Forstå Bellman-Ford](#j8-forstå-bellman-ford)
- [x] [J9] [Forstå Dag-Shortest-Paths](#j9-forstå-dag-shortest-paths)
- [ ] ! [J10] [Forstå kobling mellom Dag-Shortest-Paths og dynamisk programmering](#-j10-forstå-kobling-mellom-dag-shortest-paths-og-dynamisk-programmering)
- [x] [J11] [Forstå Dijkstra](#j11-forstå-dijkstra)

### Forelesning 11: Korteste vei fra alle til alle
- [ ] [K1] [Forstå forgjengerstrukturen for alle-til-alle-varianten av korteste vei-problemet](#k1-forstå-forgjengerstrukturen-for-alle-til-alle-varianten-av-korteste-vei-problemet-print-all-pairs-shortest-path)
- [x] [K2] [Forstå Floyd-Warshall](#k2-forstå-floyd-warshall)
- [x] [K3] [Forstå Transitive-Closure](#k3-forstå-transitive-closure)
- [ ] [K4] [Forstå Johnson](#k4-forstå-johnson)

### Forelesning 12: Maksimal flyt
- [ ] [L1] [Kunne definere flytnett, flyt og maks-flyt-problemet](#l1-kunne-definere-flytnett-flyt-og-maks-flyt-problemet)
- [ ] [L2] [Kunne håndtere antiparallelle kanter og flere kilder og sluk](#l2-kunne-håndtere-antiparallelle-kanter-og-flere-kilder-og-sluk)
- [ ] ! [L3] [Kunne definere restnettet til et flytnett med en gitt flyt](#-l3-kunne-definere-restnettet-til-et-flytnett-med-en-gitt-flyt)
- [ ] [L4] [Forstå hvordan man kan oppheve (cancel) flyt](#l4-forstå-hvordan-man-kan-oppheve-cancel-flyt)
- [ ] [L5] [Forstå hva en forøkende sti (augmenting path) er](#l5-forstå-hva-en-forøkende-sti-augmenting-path-er)
- [ ] [L6] [Forstå hva snitt, snitt-kapasitet og minimalt snitt er](#l6-forstå-hva-snitt-snitt-kapasitet-og-minimalt-snitt-er)
- [ ] ! [L7] [Forstå maks-flyt/min-snitt-teoremet](#-l7-forstå-maks-flytmin-snitt-teoremet)
- [x] [L8] [Forstå Ford-Fulkerson-Method og Ford-Fulkerson](#l8-forstå-ford-fulkerson-method-og-ford-fulkerson)
- [ ] [L9] [Vite at Ford-Fulkerson med BFS kalles Edmonds-Karp-algoritmen](#l9-vite-at-ford-fulkerson-med-bfs-kalles-edmonds-karp-algoritmen)
- [ ] [L10] [Forstå hvordan maks-flyt kan finne en maksimum bipartitt matching](#l10-forstå-hvordan-maks-flyt-kan-finne-en-maksimum-bipartitt-matching)
- [ ] ! [L11] [Forstå heltallsteoremet (integrality theorem)](#-l11-forstå-heltallsteoremet-integrality-theorem)

### Forelesning 13: NP-kompletthet
- [ ] [M1] [Forstå sammenhengen mellom optimerings- og beslutnings-problemer](#m1-forstå-sammenhengen-mellom-optimerings--og-beslutnings-problemer)
- [ ] [M2] [Forstå koding (encoding) av en instans](#m2-forstå-koding-encoding-av-en-instans)
- [ ] [M3] [Forstå hvorfor løsningen på det binære ryggsekkproblemet ikke er polynomisk](#m3-forstå-hvorfor-løsningen-på-det-binære-ryggsekkproblemet-ikke-er-polynomisk)
- [ ] [M4] [Forstå forskjellen på konkrete og abstrakte problemer](#m4-forstå-forskjellen-på-konkrete-og-abstrakte-problemer)
- [ ] [M5] [Forstå representasjonen av beslutningsproblemer som formelle språk](#m5-forstå-representasjonen-av-beslutningsproblemer-som-formelle-språk)
- [ ] [M6] [Forstå definisjonen av klassene P, NP og co-NP](#m6-forstå-definisjonen-av-klassene-p-np-og-co-np)
- [ ] [M7] [Forstå redusibilitets-relasjonen $\leq_p$](#m7-forstå-redusibilitets-relasjonen-leq_p)
- [ ] ! [M8] [Forstå definisjonen av NP-hardhet og NP-kompletthet](#-m8-forstå-definisjonen-av-np-hardhet-og-np-kompletthet)
- [ ] [M9] [Forstå den konvensjonelle hypotesen om forholdet mellom P, NP og NPC](#m9-forstå-den-konvensjonelle-hypotesen-om-forholdet-mellom-p-np-og-npc)
- [ ] ! [M10] [Forstå hvordan NP-kompletthet kan bevises ved én reduksjon](#-m10-forstå-hvordan-np-kompletthet-kan-bevises-ved-én-reduksjon)
- [ ] ! [M11] [Kjenne de NP-komplette problemene CIRCUIT-SAT, SAT, 3-CNF-SAT, CLIQUE, VERTEX-COVER, HAM-CYCLE, TSP og SUBSET-SUM](#-m11-kjenne-de-np-komplette-problemene-circuit-sat-sat-3-cnf-sat-clique-vertex-cover-ham-cycle-tsp-og-subset-sum)
- [ ] [M12] [Forstå at det binære ryggsekkproblemet er NP-hardt](#m12-forstå-at-det-binære-ryggsekkproblemet-er-np-hardt)
- [ ] [M13] [Forstå at lengste enkle-vei-problemet er NP-hardt](#m13-forstå-at-lengste-enkle-vei-problemet-er-np-hardt)
- [ ] [M14] [Være i stand til å konstruere enkle NP-kompletthetsbevis](#m14-være-i-stand-til-å-konstruere-enkle-np-kompletthetsbevis)

## Realisert

### Forelesning 1: Problemer og algoritmer
#### [A1] Forstå bokas pseudokode-konvensjoner
#### [A2] Kjenne egenskapene til random-access machine-modellen (RAM)
#### [A3] Kunne definere problem, instans og problemstørrelse
#### ! [A4] Kunne definere asymptotisk notasjon, O, Ω, Θ, o og ω.
#### ! [A5] Kunne definere best-case, average-case og worst-case
#### ! [A6] Forstå løkkeinvarianter og induksjon
#### ! [A7] Forstå rekursiv dekomponering og induksjon over delinstanser
#### [A8] Forstå Insertion-Sort

##### Insertion-Sort
- Time Complexity:
  - Worst case: $\Theta(n^2)$, if input is in reverse order
  - Average case: $\Theta(n^2)$
  - Best case: $\Theta(n)$, if input is already sorted
- Space Complecity: $\Theta(1)$

In place: True
Stable: True
Loop invariant: At the start of each iteration, the subarray $A[1 \dots j - 1]$
consists of the elements originally in $A[1 \dots j - 1]$, but in sorted order.

<details>
    <summary>Kode for Insertion-Sort</summary>

````python
def insertion_sort(A):
    for j in range(1, len(A)):
        key = A[j]
        # Insert A[j] into the sorted sequence A[0 .. j - 1]
        i = j - 1
        while i >= 0 and A[i] > key:
            A[i + 1] = A[i]
            i -= 1
        A[i + 1] = key
````
[Implementasjon av Insertion-Sort](/lib/insertion_sort.py)
</details>

### Forelesning 2: Datastrukturer

#### [B1] Forstå hvordan stakker og køer fungerer

##### Stakker

<details>
    <summary>Kode for stakker</summary>
    
````python
def stack_empty(S):
    return S.top == 0

def push(S, x):
    S.top += 1
    S[S.top] = x

def pop(S):
    if stack_empty(S):
        print("ERROR: underflow")
    else:
        S.top -= 1
        return S[S.top + 1]
````
[Implementasjon av stakker](lib/structures/stack.py)
</details>

##### Køer

<details>
    <summary>Kode for køer</summary>
    
````python
def enqueue(Q, x):
    Q[Q.tail] = x
    if Q.tail == Q.length:
        Q.tail = 0
    else:
        Q.tail += 1

def dequeue(Q):
    x = Q[Q.head]
    if Q.head == Q.length:
        Q.head = 0
    else:
        Q.head += 1
    return x
````
[Implementasjon av køer](lib/structures/queue.py)
</details>

#### [B2] Forstå hvordan lenkede lister fungerer
(List-Search, List-Insert, List-Delete, List-Delete', List-Search',
List-Insert')

<details>
    <summary>Kode for lenkede lister</summary>

````python
def list_search(L, k):
    x = L.head
    while x != None and x.key != k:
        x = x.next
    return x

def list_insert(L, x):
    x.next = L.head
    if L.head != None:
        L.head.prev = x
    L.head = x
    x.prev = None

def list_delete(L, x):
    if x.prev != None:
        x.prev.next = x.next
    else:
        L.head = x.next
    if x.next != None:
        x.next.prev = x.prev
````
[Implementasjon av lenkede lister](lib/structures/linked_list.py)
</details>

#### [B3] Forstå hvordan pekere og objekter kan implementeres
#### ! [B4] Forstå hvordan direkte adressering og hashtabeller fungerer

##### Direkte adressering (er dette samme som open address?)
<details>
    <summary>Kode</summary>
    
````python
def hash_insert(T, k):
    i = 0
    j = h(k, i)
    if T[j] == None:
        T[j] = k
        return j
    i += 1
    while i != T.m:
        j = h(k, i)
        if T[j] == None:
            T[j] = k
            return j
        i += 1

def hash_search(T, k):
    i = 0
    j = h(k, i)
    if T[j] == k:
        return j
    i += 1
    while T[j] != None and i != T.m:
        j = h(k, i)
        if T[j] == k:
            return j
        i += 1
    return None
````
[Implementasjon av open address hash table](lib/structures/hash_table_open_address.py)
</details>

#### [B5] Forstå konfliktløsing ved kjeding (chaining)
<details>
    <summary>Kode</summary>
    
````python
def chained_hash_insert(T, x):
    T[h(x.key)].append(x)

def chained_hash_search(T, k):
    for x in T[h(k)]:
        if x.key == k:
            return x
    return None

def chained_hash_delete(T, x):
    T[h(x.key)].remove(x)
````
[Implementasjon av kjeding](lib/structures/hash_table_chained.py)
</details>

#### [B6] Kjenne til grunnleggende hashfunksjoner
#### [B7] Vite at man for statiske datasett kan ha worst-case O(1) for søk
#### [B8] Kunne definere amortisert analyse
#### [B9] Forstå hvordan dynamiske tabeller fungerer
<details>
    <summary>Kode</summary>
    
````python
def table_insert(T, x):
    if T.size == 0:
        T.table = [None]
        T.size = 1
    if T.num == T.size:
        new_table = [None] * (T.size * 2)
        for i in range(len(T.table)):
            new_table[i] = T.table[i]
        T.table = new_table
        T.size *= 2
    T.table[T.num] = x
    T.num += 1

def table_delete(T, x):
    T.table[T.table.index(x)] = None
    T.num -= 1
    if T.num < T.size//4:
        new_table = [None] * (T.size//2)
        j = 0
        for i in range(len(T.table)):
            if T.table[i] != None:
                new_table[j] = T.table[i]
                j += 1
        T.table = new_table
        T.size //= 2
````
[Implementasjon av dynamiske tabeller](lib/structures/table.py)
</details>

### Forelesning 3: Splitt og hersk
#### ! [C1] Forstå designmetoden divide-and-conquer (splitt og hersk)
#### [C2] Forstå maximum-subarray-problemet med løsninger
##### Find-Maximum-Subarray

Category: Divide and conquer

Time Complexity: $\Theta(n\lg{n})$
Space Complecity: $\Theta(1)$
<details>
    <summary>Kode</summary>
    
````python
def find_max_crossing_subarray(A, low, mid, high):
    left_sum = -float('inf')
    _sum = 0
    for i in range(mid, low - 1, -1):
        _sum += A[i]
        if _sum > left_sum:
            left_sum = _sum
            max_left = i
    right_sum = -float('inf')
    _sum = 0
    for j in range(mid + 1, high + 1):
        _sum += A[j]
        if _sum > right_sum:
            right_sum = _sum
            max_right = j
    return max_left, max_right, left_sum + right_sum

def find_maximum_subarray(A, low, high):
    if high == low:
        return (low, high, A[low])
    else:
        mid = floor((low + high) / 2)
        left_low, left_high, left_sum = find_maximum_subarray(A, low, mid)
        right_low, right_high, right_sum = find_maximum_subarray(A, mid + 1, high)
        cross_low, cross_high, cross_sum = find_max_crossing_subarray(A, low, mid, high)
        if left_sum >= right_sum and left_sum >= cross_sum:
            return left_low, left_high, left_sum
        elif right_sum >= left_sum and right_sum >= cross_sum:
            return right_low, right_high, right_sum
        else:
            return cross_low, cross_high, cross_sum
````
[Implementasjon maximum-subarray-problemet](lib/find_maximum_subarray.py)
</details>

#### [C3] Forstå Bisect og Bisect'

##### Bisect
- Time Complexity:
  - Worst case: $\Theta(1)$ if v is in the middle
  - Average case: $\Theta(\lg{n})$
  - Best case: $\Theta(\lg{n})$

Space Complecity: $\Theta(1)$
<details>
    <summary>Kode</summary>
    
````python
def bisect(A, p, r, v):
    if p <= r:
        q = floor((p + r) / 2)
        if v == A[q]:
            return q
        elif v < A[q]:
            return bisect(A, p, q - 1, v)
        else:
            return bisect(A, q + 1, r, v)
    else:
        return None
````
[Implementasjon av Bisect](lib/bisect.py)
</details>

##### Iterative-Bisect (BINARY-SEARCH)
<details>
    <summary>Kode</summary>
    
````python
def iterative_bisect(A, p, r, v):
    while p <= r:
        q = floor((p + r) / 2)
        if v == A[q]:
            return q
        elif v < A[q]:
            r = q - 1
        else:
            p = q + 1
    return None
````
[Implementasjon av Iterative-Bisect](lib/bisect.py)
</details>

#### [C4] Forstå Merge-Sort
Time Complexity:
    Worst case: $\Theta(n \lg n)$
    Average case: $\Theta(n \lg n)$
    Best case: $\Theta(n \lg n)$
Space Complecity: $\Theta(n)$
In place: False
Stable: True
<details>
    <summary>Kode</summary>
    
````python
def merge(A, p, q, r):
    n1 = q - p + 1
    n2 = r - q
    L = [0] * (n1 + 1)
    R = [0] * (n2 + 1)
    for i in range(n1):
        L[i] = A[p + i]
    for j in range(n2):
        R[j] = A[q + j + 1]
    L[n1] = float('inf')
    R[n2] = float('inf')
    i = 0
    j = 0
    for k in range(p, r + 1):
        if L[i] <= R[j]:
            A[k] = L[i]
            i += 1
        else:
            A[k] = R[j]
            j += 1

def merge_sort(A, p, r):
    if p < r:
        q = floor((p + r) / 2)
        merge_sort(A, p, q)
        merge_sort(A, q + 1, r)
        merge(A, p, q, r)
````
[Implementasjon Merge-Sort](lib/merge_sort.py)
</details>

#### [C5] Forstå Quicksort og Randomized-Quicksort
##### Quicksort
Time Complexity:
    Worst case: Theta(n^2) if input is sorted
    Average case: Theta(n lg n)
    Best case: Theta(n lg n)
Space Complecity: Theta(1)

In place: True
Stable: False
<details>
    <summary>Kode</summary>
    
````python
def partition(A, p, r):
    x = A[r]
    i = p - 1
    for j in range(p, r):
        if A[j] <= x:
            i += 1
            A[i], A[j] = A[j], A[i]
    A[i+1], A[r] = A[r], A[i+1]
    return i + 1

def quicksort(A, p, r):
    if p < r:
        q = partition(A, p, r)
        quicksort(A, p, q - 1)
        quicksort(A, q + 1, r)
````
[Implementasjon av Quicksort](lib/quicksort.py)
</details>

##### Randomized-Quicksort
Time Complexity:
    Expected worst case: Theta(n lg n)
    Average case: Theta(n lg n)
    Best case: Theta(n lg n)
Space Complecity: Theta(1)
In place: True
Stable: False
<details>
    <summary>Kode</summary>
    
````python
def randomized_partition(A, p, r):
    i = randint(p, r)
    A[i], A[r] = A[r], A[i]
    return partition(A, p, r)

def randomized_quicksort(A, p, r):
    if p < r:
        q = randomized_partition(A, p, r)
        randomized_quicksort(A, p, q - 1)
        randomized_quicksort(A, q + 1, r)
````
[Implementasjon av Randomized-Quicksort](lib/randomized_select.py)
</details>

#### ! [C6] Kunne løse rekurrenser med substitusjon, rekursjonstrær og masterteoremet
#### ! [C7] Kunne løse rekurrenser med iterasjonsmetoden
#### [C8] Forstå hvordan variabelskifte fungerer

### Forelesning 4: Rangering i lineær tid
#### ! [D1] Forstå hvorfor sammenligningsbasert sortering har en worst-case på Ω(n lg n)
#### [D2] Vite hva en stabil sorteringsalgoritme er
#### [D3] Forstå Counting-Sort, og hvorfor den er stabil
Time Complexity: Theta(n + k)
Space Complecity: Theta(k)
In place: False
Stable: True
<details>
    <summary>Kode</summary>
    
````python
def counting_sort(A, B, k):
    
    C = [0] * k
    for i in A:
        C[i] += 1
    for i in range(1, k):
        C[i] += C[i-1]
    for i in range(len(A) - 1, -1, -1):
        v = A[i]
        B[C[v]-1] = v
        C[v] -= 1
````
[Implementasjon av Counting-Sort](lib/counting_sort.py)
</details>

#### ! [D4] Forstå Radix-Sort, og hvorfor den trenger en stabil subrutine
(using COUNTING-SORT)
Time Complexity: Theta(d(n + k))
Space Complecity: Theta(n + k)
In place: False
Stable: True
<details>
    <summary>Kode</summary>
    
````python
def counting_sort(A, B, k, d):
    C = [0] * k
    for v in A:
        C[ord(v[d])] += 1
    for i in range(1, k):
        C[i] += C[i-1]
    for i in range(len(A) - 1, -1, -1):
        v = A[i]
        B[C[ord(v[d])]-1] = v
        C[ord(v[d])] -= 1

def radix_sort(A, d):
    B = [None] * len(A)
    k = (ord(max(c for word in A for c in word)) + 1) \
        if len(A) and d else 0
    for i in range(d-1,-1,-1):
        counting_sort(A, B, k, i)
        A, B = B, A
    return A
````
[Implementasjon av Radix-Sort](lib/radix_sort.py)
</details>

#### [D5] Forstå Bucket-Sort
Time Complexity:
    Worst case: Theta(n^2) if many elements are placed in the same bucket
    Average case: Theta(n)
    Best case: Theta(n)
Space Complecity: Theta(n)

In place: False
Stable: True
<details>
    <summary>Kode</summary>
    
````python
def bucket_sort(A):
    n = len(A)
    B = [[] for _ in range(n)]

    for i in range(n):
        B[floor(n * A[i])].append(A[i])

    for j in range(n):
        insertion_sort(B[j])

    return [value for bucket in B for value in bucket]
````
[Implementasjon av Bucket-Sort](lib/bucket_sort.py)
</details>

#### [D6] Forstå Randomized-Select
Returns the $i$th smallest element of the array $A[p \dots r]$.

Time Complexity:
    Worst case: Theta(n^2)
    Average case: Theta(n)
    Best case: Theta(n)
Space Complecity: Theta(1)
In place: True
<details>
    <summary>Kode</summary>
    
````python
def randomized_select(A, p, r, i):
    if p == r:
        return A[p]
    q = randomized_partition(A,p,r)
    k = q - p + 1
    if i == k:
        return A[q]
    elif i < k:
        return randomized_select(A, p, q - 1, i)
    else:
        return randomized_select(A, q + 1, r, i - k)
````
[Implementasjon av Randomized-Select](lib/randomized_select.py)
</details>

#### [D7] Kjenne til Select
Merk: Det kreves ikke grundig forståelse av virkemåten til Select.

### Forelesning 5: Rotfaste trestrukturer
#### ! [E1] Forstå hvordan heaps fungerer, og hvordan de kan brukes som prioritetskøer
(Parent, Left, Right, Max-Heapify, Build-Max-Heap, Heapsort,
Max-Heap-Insert, Heap-Extract-Max, Heap-Increase-Key, Heap-Maximum. Også
tilsvarende for min-heaps, f.eks., Build-Min-Heap og Heap-Extract-Min.)
##### Heap
<details>
    <summary>Kode</summary>
    
````python
def parent(i):
    return floor(i / 2)

def left(i):
    return 2 * i + 1 # 2 * i, if 1-indexed

def right(i):
    return 2 * i + 2  # 2 * i + 1, if 1-indexed
````
</details>

##### Max-Heap
<details>
    <summary>Kode</summary>
    
````python
def max_heapify(A, i):
    """
    MAX-HEAPIFY
    Time complexity: O(lg n)
    """
    l = left(i)
    r = right(i)
    if l < A.heap_size and A[l] > A[i]:
        largest = l
    else:
        largest = i
    if r < A.heap_size and A[r] > A[largest]:
        largest = r
    if largest != i:
        A[i], A[largest] = A[largest], A[i]
        max_heapify(A, largest)

def build_max_heap(A):
    """
    BUILD-MAX-HEAP
    Time complexity: O(n)
    """
    A.heap_size = len(A)
    for i in range(floor(len(A) / 2), -1, -1):
        max_heapify(A, i)

def heap_maximum(A):
    return A[0]

def heap_extract_max(A):
    """
    HEAP-EXTRACT-MAX
    Time complexity: O(lg n)
    """
    if A.heap_size < 0:
        print("ERROR: heap underflow")
    max_ = A[0]
    A[0] = A[A.heap_size - 1]
    A.heap_size -= 1
    max_heapify(A, 0)
    return max_

def heap_increase_key(A, i, key):
    """
    HEAP-INCREASE-KEY
    Time complexity: O(lg n)
    """
    if key < A[i]:
        print("ERROR: new key is smaller than current key")
    A[i] = key
    while i > 0 and A[parent(i)] < A[i]:
        A[i], A[parent(i)] = A[parent(i)], A[i]
        i = parent(i)

def max_heap_insert(A, key):
    A.heap_size += 1
    A[A.heap_size - 1] = -float('inf')
    heap_increase_key(A, A.heap_size - 1, key)
````
</details>

##### Min-Heap
<details>
    <summary>Kode</summary>
    
````python
def min_heapify(A, i):
    """
    MIN-HEAPIFY
    Time complexity: O(lg n)
    """
    l = left(i)
    r = right(i)
    if l < A.heap_size and A[l] < A[i]:
        smallest = l
    else:
        smallest = i
    if r < A.heap_size and A[r] < A[smallest]:
        smallest = r
    if smallest != i:
        A[i], A[smallest] = A[smallest], A[i]
        min_heapify(A, smallest)

def build_min_heap(A):
    """
    BUILD-MIN-HEAP
    Time complexity: O(n lg n)
    """
    A.heap_size = len(A)
    for i in range(floor(len(A) / 2), -1, -1):
        min_heapify(A, i)

def heap_minimum(A):
    return A[0]

def heap_extract_min(A):
    """
    HEAP-EXTRACT-MIN
    Time complexity: O(lg n)
    """
    if A.heap_size < 0:
        print("ERROR: heap underflow")
    min_ = A[0]
    A[0] = A[A.heap_size - 1]
    A.heap_size -= 1
    min_heapify(A, 0)
    return min_

def heap_decrease_key(A, i, key):
    """
    HEAP-DECREASE-KEY
    Time complexity: O(lg n)
    """
    if A[i] < key:
        print("ERROR: new key is larger than current key")
    A[i] = key
    while i > 0 and A[i] < A[parent(i)]:
        A[i], A[parent(i)] = A[parent(i)], A[i]
        i = parent(i)

def min_heap_insert(A, key):
    A.heap_size += 1
    A[A.heap_size - 1] = float('inf')
    heap_decrease_key(A, A.heap_size - 1, key)
````
[Implementasjon av heaps](lib/structures/binary_heap.py)
</details>

#### [E2] Forstå Heapsort
<details>
    <summary>Kode</summary>
    
````python
def heapsort(A):
    """
    HEAPSORT

    Time Complexity: O(n lg n)
    Space Complecity: Theta(1)

    In place: True
    Stable: False
    """
    build_max_heap(A)
    for i in range(len(A) - 1, 0, -1):
        A[0], A[i] = A[i], A[0]
        A.heap_size -= 1
        max_heapify(A, 0)
````
[Implementasjon av Heapsort](lib/heapsort.py)
</details>

#### [E3] Forstå hvordan rotfaste trær kan implementeres
#### ! [E4] Forstå hvordan binære søketrær fungerer
<details>
    <summary>Kode</summary>
    
````python
def inorder_tree_walk(x):
    if x != None:
        inorder_tree_walk(x.left)
        print(x.key)
        inorder_tree_walk(x.right)

def tree_search(x, k):
    if x == None or k == x.key:
        return x
    if k < x.key:
        return tree_search(x.left, k)
    else:
        return tree_search(x.right, k)

def iterative_tree_search(x, k):
    while x != None and k != x.key:
        if k < x.key:
            x = x.left
        else:
            x = x.right
    return x

def tree_minimum(x):
    while x.left != None:
        x = x.left
    return x

def tree_maximum(x):
    while x.right != None:
        x = x.right
    return x

def tree_successor(x):
    if x.right != None:
        return tree_minimum(x.right)
    y = x.p
    while y != None and x == y.right:
        x = y
        y = y.p
    return y

def tree_insert(T, z):
    y = None
    x = T.root
    while x != None:
        y = x
        if z.key < x.key:
            x = x.left
        else:
            x = x.right
    z.p = y
    if y == None:
        T.root = z
    elif z.key < y.key:
        y.left = z
    else:
        y.right = z

def transplant(T, u, v):
    if u.p == None:
        T.root = v
    elif u == u.p.left:
        u.p.left = v
    else:
        u.p.right = v
    if v != None:
        v.p = u.p

def tree_delete(T, z):
    if z.left == None:
        transplant(T, z, z.right)
    elif z.right == None:
        transplant(T, z, z.left)
    else:
        y = tree_minimum(z.right)
        if y.p != z:
            transplant(T, y, y.right)
            y.right = z.right
            y.right.p = y
        transplant(T, z, y)
        y.left = z.left
        y.left.p = y
````
[Implementasjon av binære søketrær](lib/structures/binary_tree.py)
</details>

#### [E5] Vite at forventet høyde for et tilfeldig binært søketre er Θ(lg n)
#### [E6] Vite at det finnes søketrær med garantert høyde på Θ(lg n)

### Forelesning 6: Dynamisk programmering
#### ! [F1] Forstå ideen om en delinstansgraf
#### ! [F2] Forstå designmetoden dynamisk programmering
#### ! [F3] Forstå løsning ved memoisering (top-down)
#### [F4] Forstå løsning ved iterasjon (bottom-up)
#### [F5] Forstå hvordan man rekonstruerer en løsning fra lagrede beslutninger
#### [F6] Forstå hva optimal delstruktur er
#### [F7] Forstå hva overlappende delinstanser er
#### [F8] Forstå eksemplene stavkutting og LCS
##### Stavkutting
<details>
    <summary>Kode</summary>
    
````python
def cut_rod(p, n):
    """
    CUT-ROD

    Time Complexity: O(2^n)
    """
    if n == 0:
        return 0
    q = -float('inf')
    for i in range(1, n + 1):
        q = max(q, p[i - 1] + cut_rod(p, n - i))
    return q

def memoized_cut_rod(p, n):
    """
    MEMOIZED-CUT-ROD

    Time Complexity: O(n^2)
    """
    r = [-float('inf')] * (n + 1)
    return memoized_cut_rod_aux(p, n, r)

def memoized_cut_rod_aux(p, n, r):
    if r[n - 1] >= 0:
        return r[n - 1]
    if n == 0:
        q = 0
    else:
        q = -float('inf')
        for i in range(1, n + 1):
            q = max(q, p[i - 1] + memoized_cut_rod_aux(p, n - i))
    r[n] = q
    return q

def bottom_up_cut_rod(p, n):
    """
    BOTTOM-UP-CUT-ROD

    Time Complexity: O(n^2)
    """
    r = [0] * (n + 1)
    for j in range(1, n + 1):
        q = -float('inf')
        for i in range(1, j + 1):
            q = max(q, p[i] + r[j - i])
        r[j] = q
    return r[n]

def extended_bottom_up_cut_rod(p, n):
    """
    EXTENDED-BOTTOM-UP-CUT-ROD

    Time Complexity: O(n^2)
    """
    r = [0] * (n + 1)
    s = [0] * (n + 1)
    for j in range(1, n + 1):
        q = -float('inf')
        for i in range(1, j + 1):
            if q < p[i] + r[j - i]:
                q = p[i] + r[j - i]
                s[j] = i
        r[j] = q
    return r, s

def print_cut_rod_solution(p, n):
    """
    PRINT-CUT-ROD-SOLUTION

    Time Complexity: O(n^2)
    """
    _, s = extended_bottom_up_cut_rod(p, n)
    while n > 0:
        print(s[n])
        n -= s[n]
````
[Implementasjon av stavkutting](lib/cut_rod.py)
</details>

##### Longest common subsequence (LCS)
<details>
    <summary>Kode</summary>
    
````python
def lcs_length(X, Y):
    """
    LCS-LENGTH

    Time Complexity: Theta(m*n)
    Space Complexity: Theta(m*n)
    """
    m = len(X)
    n = len(Y)
    b = [[0] * n for _ in range(m)]
    c = [[0] * (n + 1) for _ in range(m + 1)]

    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if X[i - 1] == Y[j - 1]:
                c[i][j] = c[i - 1][j - 1] + 1
                b[i - 1][j - 1] = '↖'
            elif c[i - 1][j] >= c[i][j - 1]:
                c[i][j] = c[i - 1][j]
                b[i - 1][j - 1] = '↑'
            else:
                c[i][j] = c[i][j - 1]
                b[i - 1][j - 1] = '←'
    return c, b

def print_lcs(b, X, i, j):
    """
    PRINT-LCS

    Time Complexity: Theta(m + n)
    Space Complexity: Theta(1)
    """
    if i == -1 or j == -1:
        return
    if b[i][j] == '↖':
        print_lcs(b, X, i - 1, j - 1)
        print(X[i], end=" ")
    elif b[i][j] == '↑':
        print_lcs(b, X, i - 1, j)
    else:
        print_lcs(b, X, i, j - 1)
````
[Implementasjon av LCS](lib/longest_common_subsequence.py)
</details>

#### [F9] Forstå løsningen på det binære ryggsekkproblemet
<details>
    <summary>Kode</summary>
    
````python
def knapsack(n, W, w, v):
    """
    KNAPSACK

    Time Complexity: Theta(nW) = Theta(n2^m)
    """
    if n == 0:
        return 0
    x = knapsack(n - 1, W)
    if w[n] > W:
        return x
    else:
        y = knapsack(n - 1, W - w[n]) + v[n]
        return max(x, y)

def bottom_up_knapsack(n, W, w, v):
    """
    KNAPSACK'

    Time Complexity: Theta(nW) = Theta(n2^m)
    """
    K = [[0] * (W + 1) for _ in range(n + 1)]
    for i in range(1, n + 1):
        for j in range(W + 1):
            x = K[i - 1][j]
            if j < w[i]:
                K[i][j] = x
            else:
                y = K[i - 1][j - w[i]] + v[i]
                K[i][j] = max(x, y)

def continous_knapsack():
    pass # TODO: implement continous knapsack
````
[Implementasjon av det binære ryggsekkproblemet](lib/knapsack.py)
</details>

### Forelesning 7: Grådige algoritmer
#### ! [G1] Forstå designmetoden grådighet
#### ! [G2] Forstå grådighetsegenskapen (the greedy-choice property)
#### [G3] Forstå eksemplene aktivitet-utvelgelse og det kontinuerlige ryggsekkproblemet
#### [G4] Forstå Huffman og Huffman-koder
<details>
    <summary>Kode</summary>
    
````python
def huffman(C):
    n = len(C)
    Q = build_min_heap(C.copy())
    for _ in range(n - 1):
        z = Node()
        x = heap_extract_min(Q)
        y = heap_extract_min(Q)
        z.freq = x.freq + y.freq
        min_heap_insert(Q, z)
    return heap_extract_min(Q)
````
[Implementasjon av Huffman](lib/huffman.py)
</details>

### Forelesning 8: Traversering av grafer
#### [H1] Forstå hvordan grafer kan implementeres
#### [H2] Forstå BFS, også for å finne korteste vei uten vekter
<details>
    <summary>Kode</summary>
    
````python
def bfs(G, s):
    for u in G.V - set(s):
        u.color = "white"
        u.d = float('inf')
        u.pi = None
    s.color = "gray"
    s.d = 0
    s.pi = None
    Q = Queue()
    enqueue(Q, s)
    while Q:
        u = dequeue(Q)
        for v in G.Adj[u]:
            if v.color == "white":
                v.color = "gray"
                v.d = u.d + 1
                v.pi = u
                enqueue(Q, v)
        u.color = "black"
````
[Implementasjon av BFS](lib/bfs.py)
</details>

#### [H3] Forstå DFS, parentesteoremet og hvit-sti-teoremet
<details>
    <summary>Kode</summary>
    
````python
def dfs(G):
    global time
    for u in G.V:
        u.color == "white"
        u.pi = None
    time = 0
    for u in G.V:
        if u.color == "white":
            dfs_visit(G, u)
    
def dfs_visit(G, u):
    global time
    time += 1
    u.d = time
    u.color = "gray"
    for v in G.Adj[u]:
        if v.color == "white":
            v.pi = u
            dfs_visit(G, v)
    u.color = "black"
    time += 1
    u.f = time
````
[Implementasjon av DFS](lib/dfs.py)
</details>

#### [H4] Forstå hvordan DFS klassifiserer kanter
#### [H5] Forstå Topological-Sort
<details>
    <summary>Kode</summary>
    
````python
def topological_sort(G):
    global L
    L = LinkedList()
    ts_dfs(G)
    return L

def ts_dfs(G):
    global time
    for u in G.V:
        u.color == "white"
        u.pi = None
    time = 0
    for u in G.V:
        if u.color == "white":
            ts_dfs_visit(G, u)
    
def ts_dfs_visit(G, u):
    global time, L
    time += 1
    u.d = time
    u.color = "gray"
    for v in G.Adj[u]:
        if v.color == "white":
            v.pi = u
            ts_dfs_visit(G, v)
    u.color = "black"
    time += 1
    u.f = time
    list_insert(L, LinkedListElement(u))
````
[Implementasjon av Topological-Sort](lib/topological_sort.py)
</details>

#### [H6] Forstå hvordan DFS kan implementeres med en stakk
#### [H7] Forstå hva traverseringstrær (som bredde-først- og dybde-først-trær) er
#### ! [H8] Forstå traversering med vilkårlig prioritetskø

### Forelesning 9: Minimale spenntrær
#### [I1] Forstå skog-implementasjonen av disjunkte mengder
<details>
    <summary>Kode</summary>
    
````python
def connected_components(G):
    for v in G.V:
        make_set(v)
    for u, v in G.E:
        if find_set(u) != find_set(v):
            union(u, v)

def same_component(u, v):
    return find_set(u) == find_set(v)
    
def make_set(x):
    x.p = x
    x.rank = 0

def union(x, y):
    link(find_set(x), find_set(y))

def link(x, y):
    if x.rank > y.rank:
        y.p = x
    else:
        x.p = y
        if x.rank == y.rank:
            y.rank += 1

def find_set(x):
    if x != x.p:
        x.p = find_set(x.p)
    return x.p
````
[Implementasjon av disjunkte mengder](lib/structures/disjunct_set.py) 
</details>

#### [I2] Vite hva spenntrær og minimale spenntrær er
#### ! [I3] Forstå Generic-MST
    GENERIC-MST(G, w):
        A = Ø
        while A does not form a spanning tree
            find an edge (u, v) that is safe for A
            A = A U {(u, v)}
        return A

#### [I4] Forstå hvorfor lette kanter er trygge kanter
#### [I5] Forstå MST-Kruskal
<details>
    <summary>Kode</summary>
    
````python
def mst_kruskal(G, w):
    A = set()
    for v in G.V:
        make_set(v)
    edges = sorted(G.E, key=lambda edge: w[edge[0]][edge[1]])
    for u, v in edges:
        if find_set(u) != find_set(v):
            A.add((u, v))
            union(u, v)
````
[Implementasjon av MST-Kruskal](lib/minimal_spanning_tree.py)
</details>

#### [I6] Forstå MST-Prim
<details>
    <summary>Kode</summary>
    
````python
def mst_prim(G, w, r):
    for u in G.V:
        u.key = float('inf')
        u.pi = None
    r.key = 0
    Q = build_min_heap(G.V.copy())
    while Q:
        u = heap_extract_min(Q)
        for v in G.Adj[u]:
            if v in Q and w[u][v] < v.key:
                v.pi = u
                v.key = w[u][v]
````
[Implementasjon av MST-Prim](lib/minimal_spanning_tree.py)
</details>

### Forelesning 10: Korteste vei fra én til alle
#### [J1] Forstå ulike varianter av korteste-vei- eller korteste-sti-problemet (Single-source, single-destination, single-pair, all-pairs)
#### [J2] Forstå strukturen til korteste-vei-problemet
#### [J3] Forstå at negative sykler gir mening for korteste enkle vei (simple path)
#### [J4] Forstå at korteste enkle vei kan løses vha. lengste enkle vei og omvendt
#### [J5] Forstå hvordan man kan representere et korteste-vei-tre
#### ! [J6] Forstå kant-slakking (edge relaxation) og Relax
<details>
    <summary>Kode</summary>
    
````python
def relax(u, v, w):
    if v.d > u.d + w[u][v]:
        v.d = u.d + w[u][v]
        v.pi = u
````
[Implementasjon av Relax](lib/single_source_shortest_path.py)
</details>

#### [J7] Forstå ulike egenskaper ved korteste veier og slakking
(Triangle inequality, upper-bound property, no-path property, convergence property,
path-relaxation property, predecessor-subgraph property)
#### [J8] Forstå Bellman-Ford
<details>
    <summary>Kode</summary>
    
````python
def bellman_ford(G, w, s):
    initialize_single_source(G, s)
    for _ in range(len(G.V) - 1):
        for u, v in G.E:
            relax(u, v, w)
    for u, v in G.E:
        if v.d > u.d + w[u][v]:
            return False
    return True
````
[Implementasjon av Bellman-Ford](lib/single_source_shortest_path.py)
</details>

#### [J9] Forstå Dag-Shortest-Paths
<details>
    <summary>Kode</summary>
    
````python
def dag_shortest_paths(G, w, s):
    initialize_single_source(G, s)
    u = topological_sort(G)
    while u != None:
        for v in G.Adj[u.key]:
            relax(u, v, w)
        u = u.next
````
[Implementasjon av Dag-Shortest-Paths](lib/single_source_shortest_path.py)
</details>

#### ! [J10] Forstå kobling mellom Dag-Shortest-Paths og dynamisk programmering
#### [J11] Forstå Dijkstra
<details>
    <summary>Kode</summary>
    
````python
def dijkstra(G, w, s):
    initialize_single_source(G, s)
    S = set()
    Q = build_min_heap(G.V.copy())
    while Q:
        u = heap_extract_min(Q)
        S.add(u)
        for v in G.Adj[u]:
            relax(u, v, w)
````
[Implementasjon av Dijkstra](lib/single_source_shortest_path.py)
</details>

### Forelesning 11: Korteste vei fra alle til alle
#### [K1] Forstå forgjengerstrukturen for alle-til-alle-varianten av korteste vei-problemet
<details>
    <summary>Kode</summary>
    
````python
def print_all_pairs_shortest_path(Pi, i, j):
    if i == j:
        print(i)
    elif Pi[i][j] == None:
        print(f"No path from {i} to {j} exists.")
    else:
        print_all_pairs_shortest_path(Pi, i, Pi[i][j])
        print(j)
````
[Implementasjon av Print-All-Pairs-Shortest-Path](lib/all_pairs_shortest_paths.py)
</details>

#### [K2] Forstå Floyd-Warshall
<details>
    <summary>Kode</summary>
    
````python
def floyd_warshall(W):
    n = len(W)
    D = [None] * (n + 1)
    D[0] = W
    for k in range(1, n + 1):
        D[k] = [[None] * n for _ in range(n)]
        for i in range(n):
            for j in range(n):
                D[k][i][j] = min(D[k - 1][i][j], \
                    D[k - 1][i][k] + D[k - 1][k][j])
    return D[n]
````
[Implementasjon av Floyd-Warshall](lib/all_pairs_shortest_paths.py)
</details>

#### [K3] Forstå Transitive-Closure
<details>
    <summary>Kode</summary>
    
````python
def transitive_closure(G):
    n = len(G.V)
    T = [None] * (n + 1)
    T[0] = [[None] * n for _ in range(n)]
    for i in range(n):
        for j in range(n):
            if i == j or (i, j) in G.E:
                T[0][i][j] = 1
            else:
                T[0][i][j] = 0
    for k in range(1, n + 1):
        T[k] = [[None] * n for _ in range(n)]
        for i in range(n):
            for j in range(n):
                T[k][i][j] = T[k - 1][i][j] or T[k - 1][i][k] and T[k - 1][k][j]
    return T[n]

def transitive_closure_optimized(G):
    n = len(G.V)
    T = [[None] * n for _ in range(n)]
    for i in range(n):
        for j in range(n):
            if i == j or (i, j) in G.E:
                T[i][j] = 1
            else:
                T[i][j] = 0
    for k in range(1, n + 1):
        for i in range(n):
            for j in range(n):
                T[i][j] = T[i][j] or T[i][k] and T[k][j]
    return T
````
[Implementasjon av Transitive-Closure](lib/transitive_closure.py)
</details>

#### [K4] Forstå Johnson
<details>
    <summary>Kode</summary>
    
````python
def johnson(G, w):
    pass # TODO: implement johnson
````
[Implementasjon av Johnson](lib/all_pairs_shortest_paths.py)
</details>

### Forelesning 12: Maksimal flyt
#### [L1] Kunne definere flytnett, flyt og maks-flyt-problemet
#### [L2] Kunne håndtere antiparallelle kanter og flere kilder og sluk
#### ! [L3] Kunne definere restnettet til et flytnett med en gitt flyt
#### [L4] Forstå hvordan man kan oppheve (cancel) flyt
#### [L5] Forstå hva en forøkende sti (augmenting path) er
#### [L6] Forstå hva snitt, snitt-kapasitet og minimalt snitt er
#### ! [L7] Forstå maks-flyt/min-snitt-teoremet
#### [L8] Forstå Ford-Fulkerson-Method og Ford-Fulkerson
<details>
    <summary>Kode</summary>
    
````
FORD-FULKERSON-METHOD(G, s, t)
    initialize flow f to 0
    while there exists an augmenting path p in the residual network Gf
        augment flow f along p
    return f

FORD-FULKERSON(G, s, t)
    for each edge (u, v) in G.E
        (u, v).f = 0
    while there exists a path p from s to t in the residual network Gf
        cf(p) = min{cf(u, v) : (u, v) is in p}
        for each edge (u, v) in p
            if (u, v) in E
                (u, v).f = (u, v).f + cf(p)
            else (u, v).f = (u, v).f - cf(p)
````
</details>

#### [L9] Vite at Ford-Fulkerson med BFS kalles Edmonds-Karp-algoritmen
<details>
    <summary>Kode</summary>
    
````python
# TODO: Implement Edmonds-Karp
````
[Implementasjon av Edmonds-Karp](lib/ford_fulkerson.py)
</details>

#### [L10] Forstå hvordan maks-flyt kan finne en maksimum bipartitt matching
#### ! [L11] Forstå heltallsteoremet (integrality theorem)

### Forelesning 13: NP-kompletthet
#### [M1] Forstå sammenhengen mellom optimerings- og beslutnings-problemer
#### [M2] Forstå koding (encoding) av en instans
#### [M3] Forstå hvorfor løsningen på det binære ryggsekkproblemet ikke er polynomisk
#### [M4] Forstå forskjellen på konkrete og abstrakte problemer
#### [M5] Forstå representasjonen av beslutningsproblemer som formelle språk
#### [M6] Forstå definisjonen av klassene P, NP og co-NP
#### [M7] Forstå redusibilitets-relasjonen $\leq_p$
#### ! [M8] Forstå definisjonen av NP-hardhet og NP-kompletthet
#### [M9] Forstå den konvensjonelle hypotesen om forholdet mellom P, NP og NPC
#### ! [M10] Forstå hvordan NP-kompletthet kan bevises ved én reduksjon
#### ! [M11] Kjenne de NP-komplette problemene CIRCUIT-SAT, SAT, 3-CNF-SAT, CLIQUE, VERTEX-COVER, HAM-CYCLE, TSP og SUBSET-SUM
#### [M12] Forstå at det binære ryggsekkproblemet er NP-hardt
#### [M13] Forstå at lengste enkle-vei-problemet er NP-hardt
#### [M14] Være i stand til å konstruere enkle NP-kompletthetsbevis